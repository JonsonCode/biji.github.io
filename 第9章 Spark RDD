1>进入spark-shell交互界面
spark-shell
2>创建intRDD
val intRDD=sc.parallelize(List(3,1,2,5,5))
intRDD转化为Arry
intRDD.collect()
3>创建stringRDD
val stringRDD=sc.parallelize(List("Apple","Orange","Banana","Grape","Apple"))
将stringRDD转化为Arry
stringRDD.collect()
4>map运算
定义addOne函数
def addOne(x:Int):Int={
return (x+1)
}

intRDD.map(addOne).colelect()
匿名函数语句
intRDD.map(x=>x+1).colletc()
匿名函数+匿名参数语句
intRDD.map(_+1).collect()
5>map字符串运算
stringRDD.map(x=>"fruit:"+x).collect
6>filter数字运算
让intRDD筛选数字小于3
intRDD.filter(x=>x<3).collect
使用匿名参数让筛选数字小于3，可使用下划线_来取代x=>x<3
intRDD.filter(_<3).collect
以上两条语句的运算结果完全相同
7>filter字符运算
筛选内含ra的字符串
stringRDD.filter(x=>x.contains("ra")).collect
distinct运算（会删除重复的元素）
删除intRDD重复的元素
intRDD.distinct().collect()
删除stringRDD重复的元素
stringRDD.distinct().collect()
randomSplit运算
以随机数的方式按照4：6的比例分割为两个RDD
val sRDD=intRDD.randomSplit(Array(0.4,0.6))
读取第一个RDD
sRDD(0).collect()
sRDD(1).collect()
groupBy运算
 val gRDD=intRDD.groupBy(
 {
 x=>{if(x%2==0)"even" else "odd"}
).collect


-------------------------------------------
accumulator累加器
创建范例RDD
val intRDD=sc.parallelize(List(3,1,2,5,5))
创建total累加器
val total=sc.accumulator(0.0)
创建num累加器
val num=sc.accumulator(0)
使用foreach传入参数i，针对每条数据执行
intRDD.foreach(i=>{
total+=i
num+=1
})
显示求和和计数
println("total="+total.value+",num="+num.value)
计算平均
val avg=total.value/num.value
-----------------------------------------------
RDD Presistence持久化
创建范例RDD
val intRddMemory=sc.parallelize(List(3,1,2,5,5))
将intRddMemory持久化
intRddMemory.persist()
取消持久化
intRddMemory.unpersist()
设置存储等级范例
设置存储等级，必须先导入相关链接库
import org.apache.spark.storage.StorageLevel
创建范例RDD
val intRddMemoryAndDisk=sc.parallelize(List(3,1,2,5,5))
设置选项为StorageLevel.MEMORY_AND_DISK
intRddMemoryAndDisk.persist(StorageLevel.MEMORY_AND_DISK)
将intRddMemoryAndDisk取消持久化
intRddMemoryAndDisk.unpersist()
-----------------------------------------------------
使用Spark创建WordCount
创建WordeCount的数据目录
mkdir -p ~/workspace/WordCount/data
cd ~/workspace/WordCount/data
gedit test.txt
输入：
Apple Apple Orange 
Banana Grape Grape
进入spark-shell
在~/workspace/WordCount/data目录下输入spark-shell
执行WordCount命令
读取文本文件
val textFile=sc.textFile("file:/home/hduser/workspace/WordCount/data/test.txt")
使用flatMap空格符分开单词，并读取每个单词
val stringRDD=textFile.flatMap(line=>line.split(" "))
val countsRDD=stringRDD.map(word=> (word, 1)).reduceByKey(_+_)
保存计算结果
countsRDD.saveAsTextFile("file:/home/hduser/workspace/WordCount/data/output")
